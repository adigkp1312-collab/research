{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning LLM Models — Practical Examples\n",
    "\n",
    "This notebook provides runnable examples for fine-tuning LLMs using:\n",
    "1. **Vertex AI** (Gemini supervised fine-tuning) — managed, no GPU required locally\n",
    "2. **Hugging Face TRL** with LoRA/QLoRA — open-source, full control\n",
    "\n",
    "See also: `docs/FINE_TUNING_LLM_GUIDE.md` for the full conceptual guide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Preparation\n",
    "\n",
    "Both approaches start with well-formatted training data. Let's create example datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Example: Fine-tuning a model to be a video production assistant\n",
    "# that gives structured ad recommendations\n",
    "\n",
    "training_examples = [\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a video production assistant specializing in ad concepts. Always respond with structured JSON containing: concept_name, target_emotion, visual_description, key_message, and duration_seconds.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Create an ad concept for a premium yoga mat brand targeting beginners.\"},\n",
    "            {\"role\": \"assistant\", \"content\": json.dumps({\n",
    "                \"concept_name\": \"First Step\",\n",
    "                \"target_emotion\": \"empowerment\",\n",
    "                \"visual_description\": \"A beginner unrolls a yoga mat in a sunlit living room. Gentle morning light. Close-ups of hands and feet finding balance. Transition to confident poses.\",\n",
    "                \"key_message\": \"Every journey starts with a single pose.\",\n",
    "                \"duration_seconds\": 30\n",
    "            }, indent=2)}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a video production assistant specializing in ad concepts. Always respond with structured JSON containing: concept_name, target_emotion, visual_description, key_message, and duration_seconds.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Create an ad concept for an organic skincare brand targeting millennials.\"},\n",
    "            {\"role\": \"assistant\", \"content\": json.dumps({\n",
    "                \"concept_name\": \"Skin Deep\",\n",
    "                \"target_emotion\": \"authenticity\",\n",
    "                \"visual_description\": \"Split screen: left side shows harsh chemical formulas, right side shows ingredients growing in nature. Camera follows a woman choosing the natural path. Ends with product close-up.\",\n",
    "                \"key_message\": \"What you put on your skin matters.\",\n",
    "                \"duration_seconds\": 15\n",
    "            }, indent=2)}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a video production assistant specializing in ad concepts. Always respond with structured JSON containing: concept_name, target_emotion, visual_description, key_message, and duration_seconds.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Create an ad concept for a fitness app targeting busy professionals.\"},\n",
    "            {\"role\": \"assistant\", \"content\": json.dumps({\n",
    "                \"concept_name\": \"5-Minute Transform\",\n",
    "                \"target_emotion\": \"motivation\",\n",
    "                \"visual_description\": \"Fast-paced montage: alarm clock, coffee, commute, desk. Then a pause — phone notification. Quick workout in office clothes. Energized return to work.\",\n",
    "                \"key_message\": \"No gym. No excuses. Just 5 minutes.\",\n",
    "                \"duration_seconds\": 30\n",
    "            }, indent=2)}\n",
    "        ]\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"Created {len(training_examples)} training examples\")\n",
    "print(f\"\\nExample format:\")\n",
    "print(json.dumps(training_examples[0], indent=2)[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as JSONL (standard format for both Vertex AI and HuggingFace)\n",
    "\n",
    "output_dir = Path(\"../datasets\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# HuggingFace format (messages)\n",
    "hf_path = output_dir / \"training_data_hf.jsonl\"\n",
    "with open(hf_path, \"w\") as f:\n",
    "    for example in training_examples:\n",
    "        f.write(json.dumps(example) + \"\\n\")\n",
    "print(f\"HuggingFace format saved to: {hf_path}\")\n",
    "\n",
    "# Vertex AI format (systemInstruction + contents with 'model' role)\n",
    "vertex_examples = []\n",
    "for example in training_examples:\n",
    "    vertex_ex = {\n",
    "        \"systemInstruction\": {\n",
    "            \"parts\": [{\"text\": example[\"messages\"][0][\"content\"]}]\n",
    "        },\n",
    "        \"contents\": []\n",
    "    }\n",
    "    for msg in example[\"messages\"][1:]:\n",
    "        role = \"model\" if msg[\"role\"] == \"assistant\" else \"user\"\n",
    "        vertex_ex[\"contents\"].append({\n",
    "            \"role\": role,\n",
    "            \"parts\": [{\"text\": msg[\"content\"]}]\n",
    "        })\n",
    "    vertex_examples.append(vertex_ex)\n",
    "\n",
    "vertex_path = output_dir / \"training_data_vertex.jsonl\"\n",
    "with open(vertex_path, \"w\") as f:\n",
    "    for example in vertex_examples:\n",
    "        f.write(json.dumps(example) + \"\\n\")\n",
    "print(f\"Vertex AI format saved to: {vertex_path}\")\n",
    "\n",
    "# Verify\n",
    "print(f\"\\nVertex AI format example:\")\n",
    "print(json.dumps(vertex_examples[0], indent=2)[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Fine-Tuning on Vertex AI (Gemini)\n",
    "\n",
    "Managed fine-tuning — no local GPU required. Requires:\n",
    "- A Google Cloud project with Vertex AI enabled\n",
    "- Training data uploaded to Google Cloud Storage\n",
    "- The `google-genai` package (`pip install google-genai`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Upload training data to GCS\n",
    "# (Run this from your terminal or uncomment below)\n",
    "#\n",
    "# gsutil cp datasets/training_data_vertex.jsonl gs://YOUR_BUCKET/fine-tuning/training_data.jsonl\n",
    "\n",
    "# Configuration\n",
    "PROJECT_ID = \"artful-striker-483214-b0\"  # Your GCP project\n",
    "LOCATION = \"us-central1\"\n",
    "TRAINING_DATA_URI = \"gs://YOUR_BUCKET/fine-tuning/training_data.jsonl\"  # Update this\n",
    "TUNED_MODEL_NAME = \"video-ad-assistant-v1\"\n",
    "\n",
    "print(f\"Project: {PROJECT_ID}\")\n",
    "print(f\"Location: {LOCATION}\")\n",
    "print(f\"Training data: {TRAINING_DATA_URI}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Launch fine-tuning job using Google Gen AI SDK (recommended)\n",
    "\n",
    "from google import genai\n",
    "from google.genai.types import HttpOptions\n",
    "\n",
    "client = genai.Client(\n",
    "    vertexai=True,\n",
    "    project=PROJECT_ID,\n",
    "    location=LOCATION,\n",
    "    http_options=HttpOptions(api_version=\"v1\"),\n",
    ")\n",
    "\n",
    "# Launch supervised fine-tuning\n",
    "tuning_job = client.tunings.tune(\n",
    "    base_model=\"gemini-2.5-flash\",\n",
    "    training_dataset={\n",
    "        \"gcs_uri\": TRAINING_DATA_URI,\n",
    "    },\n",
    "    config={\n",
    "        \"tuned_model_display_name\": TUNED_MODEL_NAME,\n",
    "        \"epoch_count\": 3,\n",
    "        \"learning_rate_multiplier\": 1.0,\n",
    "    },\n",
    ")\n",
    "\n",
    "print(f\"Tuning job started: {tuning_job.name}\")\n",
    "print(f\"State: {tuning_job.state}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Monitor the tuning job\n",
    "\n",
    "import time\n",
    "\n",
    "while not tuning_job.has_ended:\n",
    "    print(f\"Status: {tuning_job.state}\")\n",
    "    time.sleep(120)  # Check every 2 minutes\n",
    "    tuning_job.refresh()\n",
    "\n",
    "print(f\"\\nTuning complete!\")\n",
    "print(f\"Tuned model: {tuning_job.tuned_model.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Test the fine-tuned model\n",
    "\n",
    "tuned_model_name = tuning_job.tuned_model.name\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=tuned_model_name,\n",
    "    contents=\"Create an ad concept for a sustainable fashion brand targeting Gen Z.\",\n",
    ")\n",
    "\n",
    "print(\"Fine-tuned model response:\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Fine-Tuning Open-Source Models with Hugging Face TRL\n",
    "\n",
    "Full control over the training process. Requires:\n",
    "- A GPU (local or cloud) — T4 (16GB) minimum for LoRA on 7B models\n",
    "- `pip install trl peft transformers datasets bitsandbytes accelerate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (uncomment to run)\n",
    "# !pip install trl peft transformers datasets bitsandbytes accelerate torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: LoRA fine-tuning with SFTTrainer\n",
    "\n",
    "from datasets import load_dataset, Dataset\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from peft import LoraConfig\n",
    "\n",
    "# Load our custom dataset\n",
    "dataset = Dataset.from_json(str(hf_path))\n",
    "print(f\"Dataset size: {len(dataset)} examples\")\n",
    "print(f\"Columns: {dataset.column_names}\")\n",
    "print(f\"\\nFirst example messages: {dataset[0]['messages'][:1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure LoRA\n",
    "peft_config = LoraConfig(\n",
    "    r=16,                        # Rank — controls adapter capacity\n",
    "    lora_alpha=16,               # Scaling factor (typically equal to r)\n",
    "    lora_dropout=0.05,           # Light regularization\n",
    "    bias=\"none\",                 # Don't train bias terms\n",
    "    target_modules=\"all-linear\", # Apply LoRA to ALL linear layers\n",
    "    task_type=\"CAUSAL_LM\",       # Causal language modeling\n",
    ")\n",
    "\n",
    "# Configure training\n",
    "training_args = SFTConfig(\n",
    "    output_dir=\"./sft-output\",\n",
    "    \n",
    "    # Training duration\n",
    "    num_train_epochs=3,           # Number of passes over the data\n",
    "    max_steps=-1,                 # -1 = use num_train_epochs\n",
    "    \n",
    "    # Batch size\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4, # Effective batch = 2 * 4 = 8\n",
    "    \n",
    "    # Learning rate\n",
    "    learning_rate=2e-4,\n",
    "    warmup_ratio=0.05,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    \n",
    "    # Memory optimization\n",
    "    bf16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    \n",
    "    # Logging\n",
    "    logging_steps=5,\n",
    "    save_steps=50,\n",
    "    save_total_limit=3,\n",
    ")\n",
    "\n",
    "print(\"Training config ready.\")\n",
    "print(f\"  LoRA rank: {peft_config.r}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  Epochs: {training_args.num_train_epochs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train (requires GPU)\n",
    "# Using a small model for demonstration — replace with your target model\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen3-0.6B\"  # Small model for testing\n",
    "# MODEL_NAME = \"meta-llama/Llama-3.1-8B\"  # Production-scale model\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=MODEL_NAME,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# Save the LoRA adapter\n",
    "trainer.save_model(\"./video-ad-assistant-lora\")\n",
    "print(\"\\nTraining complete! Adapter saved to ./video-ad-assistant-lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference with the fine-tuned LoRA model\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "# Load base model + LoRA adapter\n",
    "base_model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "model = PeftModel.from_pretrained(base_model, \"./video-ad-assistant-lora\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Generate\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a video production assistant specializing in ad concepts. Always respond with structured JSON.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Create an ad concept for a sustainable fashion brand targeting Gen Z.\"},\n",
    "]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "outputs = model.generate(inputs, max_new_tokens=256)\n",
    "response = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "print(\"Fine-tuned model response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Merge LoRA adapter into base model for production\n",
    "# This eliminates any inference overhead from the adapter\n",
    "\n",
    "merged_model = model.merge_and_unload()\n",
    "merged_model.save_pretrained(\"./video-ad-assistant-merged\")\n",
    "tokenizer.save_pretrained(\"./video-ad-assistant-merged\")\n",
    "print(\"Merged model saved to ./video-ad-assistant-merged\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: QLoRA Example (4-bit Quantization)\n",
    "\n",
    "For fine-tuning larger models (e.g., 70B) on limited hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from peft import LoraConfig\n",
    "\n",
    "# 4-bit quantization config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",            # NormalFloat4 quantization\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16, # Compute in bf16\n",
    "    bnb_4bit_use_double_quant=True,        # Double quantization for extra savings\n",
    ")\n",
    "\n",
    "# LoRA config (same as before)\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    target_modules=\"all-linear\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Training config — smaller batch size for memory efficiency\n",
    "training_args = SFTConfig(\n",
    "    output_dir=\"./qlora-output\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=1,     # Smaller batch for large models\n",
    "    gradient_accumulation_steps=8,      # Effective batch = 1 * 8 = 8\n",
    "    learning_rate=2e-4,\n",
    "    bf16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    logging_steps=5,\n",
    ")\n",
    "\n",
    "# Train with QLoRA\n",
    "trainer = SFTTrainer(\n",
    "    model=\"meta-llama/Llama-3.1-8B\",  # Or any model\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    model_init_kwargs={\"quantization_config\": bnb_config},\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(\"./qlora-adapter\")\n",
    "print(\"QLoRA training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Which Approach to Use?\n",
    "\n",
    "| Scenario | Recommended Approach |\n",
    "|----------|---------------------|\n",
    "| Quick experiment, small data | Vertex AI managed fine-tuning |\n",
    "| Production Gemini deployment | Vertex AI managed fine-tuning |\n",
    "| Full control over training | HuggingFace TRL + LoRA |\n",
    "| Large model, limited GPU | HuggingFace TRL + QLoRA |\n",
    "| Maximum accuracy, large data | Full fine-tuning (multi-GPU) |\n",
    "| This project (Vertex AI stack) | **Vertex AI managed fine-tuning** |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
